{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "878cac81-8e66-4201-ab8e-d697421b8182",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopper:\n",
    "    def __init__(self, patience=1, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ae23b04-8bd7-4555-be04-c4c4d360b172",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.distributions as td\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from SinkhornDistance import SinkhornDistance\n",
    "\n",
    "class DisentangledVAE:\n",
    "    def __init__(\n",
    "        self, n_epochs, input_dimension, latent_dimension, hidden_layer_width, number_of_labels=3, weight=[1,1], device=None\n",
    "    ):\n",
    "        self.n_epochs=n_epochs\n",
    "        self.hidden_layer_width = hidden_layer_width\n",
    "        self.input_dimension = input_dimension\n",
    "        self.latent_dimension = latent_dimension\n",
    "        self.number_of_labels = number_of_labels #supervised dimension\n",
    "        self.pred_weight = weight\n",
    "        self.beta = 1\n",
    "        self.recon_weight = 0.1\n",
    "        self.KL_weight=0.1\n",
    "        self.z_var = 1\n",
    "        self.reg_weight = 1\n",
    "        self.wasserstein=1\n",
    "        self.decoder = nn.Sequential(\n",
    "            torch.nn.Linear(self.latent_dimension, self.hidden_layer_width),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_layer_width, self.hidden_layer_width),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_layer_width, self.hidden_layer_width),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_layer_width, self.input_dimension),\n",
    "        ).to(device)\n",
    "        self.encoder = nn.Sequential(\n",
    "            torch.nn.Linear(self.input_dimension, self.hidden_layer_width),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_layer_width, self.hidden_layer_width),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_layer_width, self.hidden_layer_width),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(self.hidden_layer_width, (2 * self.latent_dimension)),\n",
    "        ).to(device)\n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.encoder.parameters()) + list(self.decoder.parameters()), lr=1e-5\n",
    "        )\n",
    "        self.mse = nn.MSELoss(reduction='mean')\n",
    "        self.device=device\n",
    "        self.batch_size = 64\n",
    "        self.early_stopper = EarlyStopper(patience=2, min_delta=0.05)\n",
    "\n",
    "        self.generate_data=False\n",
    "        self.kl_loss = nn.KLDivLoss(reduction=\"batchmean\", log_target=True)\n",
    "        self.K=10\n",
    "        self.sinkhorn = SinkhornDistance(eps=0.1, \n",
    "                                         max_iter=100, \n",
    "                                         device= device, \n",
    "                                         reduction=None)\n",
    "        self.plot=False\n",
    "        #deep fake loss\n",
    "        self.sample_ratio=1\n",
    "        self.generate_data=False\n",
    "\n",
    "    def weights_init(self, layer):\n",
    "        if type(layer) == nn.Linear:\n",
    "            torch.nn.init.orthogonal_(layer.weight)\n",
    "\n",
    "    def matrix_log_density_gaussian(self, x, mu, logvar):\n",
    "        # broadcast to get probability of x given any instance(row) in (mu,logvar)\n",
    "        # [k,:,:] : probability of kth row in x from all rows in (mu,logvar)\n",
    "        x = x.view(self.batch_size, 1, self.latent_dimension)\n",
    "        mu = mu.view(1, self.batch_size, self.latent_dimension)\n",
    "        logvar = logvar.view(1, self.batch_size, self.latent_dimension)\n",
    "        return td.Normal(loc=mu, scale=(torch.exp(logvar)) ** 0.5).log_prob(x)\n",
    "\n",
    "    def log_importance_weight_matrix(self):\n",
    "        \"\"\"\n",
    "        Calculates a log importance weight matrix\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            number of training images in the batch\n",
    "        dataset_size: int\n",
    "        number of training images in the dataset\n",
    "        \"\"\"\n",
    "        N = self.n_data\n",
    "        M = self.batch_size - 1\n",
    "        strat_weight = (N - M) / (N * M)\n",
    "        W = torch.Tensor(self.batch_size, self.batch_size).fill_(1 / M)\n",
    "        W.view(-1)[:: M + 1] = 1 / N\n",
    "        W.view(-1)[1 :: M + 1] = strat_weight\n",
    "        W[M - 1, 0] = strat_weight\n",
    "        return W.log()\n",
    "\n",
    "    def get_log_qz_prodzi(self, latent_sample, latent_dist, is_mss=True):\n",
    "        mat_log_qz = self.matrix_log_density_gaussian(\n",
    "            latent_sample,\n",
    "            latent_dist[..., : self.latent_dimension],\n",
    "            latent_dist[..., self.latent_dimension :],\n",
    "        )\n",
    "        if is_mss:\n",
    "            # use stratification\n",
    "            log_iw_mat = self.log_importance_weight_matrix().to(\n",
    "                latent_sample.device\n",
    "            )\n",
    "            mat_log_qz = mat_log_qz + log_iw_mat.view(self.batch_size, self.batch_size, 1)\n",
    "            log_qz = torch.logsumexp(\n",
    "                log_iw_mat + mat_log_qz.sum(2), dim=1, keepdim=False\n",
    "            )\n",
    "            log_prod_qzi = torch.logsumexp(\n",
    "                log_iw_mat.view(self.batch_size, self.batch_size, 1) + mat_log_qz,\n",
    "                dim=1,\n",
    "                keepdim=False,\n",
    "            ).sum(1)\n",
    "\n",
    "        else:\n",
    "            log_prod_qzi = (\n",
    "                torch.logsumexp(\n",
    "                    mat_log_qz, dim=1, keepdim=False\n",
    "                )  # sum of probabilities in each latent dimension\n",
    "                - math.log(self.batch_size * self.n_data)\n",
    "            ).sum(1)\n",
    "            log_qz = torch.logsumexp(\n",
    "                mat_log_qz.sum(2),  # sum of probabilities across all latent dimensions\n",
    "                dim=1,\n",
    "                keepdim=False,\n",
    "            ) - math.log(self.batch_size * self.n_data)\n",
    "\n",
    "        return log_qz, log_prod_qzi\n",
    "\n",
    "    def _kl_normal_loss(self, mean, logvar):\n",
    "        \"\"\"\n",
    "        Calculates the KL divergence between a normal distribution\n",
    "        with diagonal covariance and a unit normal distribution.\n",
    "        Parameters\n",
    "        ----------\n",
    "        mean : torch.Tensor\n",
    "            Mean of the normal distribution. Shape (batch_size, latent_dim) where\n",
    "            D is dimension of distribution.\n",
    "        logvar : torch.Tensor\n",
    "            Diagonal log variance of the normal distribution. Shape (batch_size,\n",
    "            latent_dim)\n",
    "        storer : dict\n",
    "            Dictionary in which to store important variables for vizualisation.\n",
    "        \"\"\"\n",
    "        latent_dim = mean.size(1)\n",
    "        # batch mean of kl for each latent dimension\n",
    "        latent_kl = 0.5 * (-1 - logvar + mean.pow(2) + logvar.exp()).mean(dim=0)\n",
    "        total_kl = latent_kl.sum()\n",
    "\n",
    "        return total_kl\n",
    "    def generate_fake(self, x1):\n",
    "        out_encoder = self.encoder(x1.to(self.device))\n",
    "        treatment = td.Independent(td.Normal(loc=out_encoder[:, :self.latent_dimension],\n",
    "                                            scale=torch.exp(out_encoder[:, self.latent_dimension :]) ** 0.5,\n",
    "                                            )\n",
    "                                   ,1)\n",
    "        x2 = treatment.rsample([self.sample_ratio]).reshape(\n",
    "            (-1,self.latent_dimension)\n",
    "        )\n",
    "        return self.decoder(x2).cpu()\n",
    "\n",
    "    def new_data(self,xhat_0,yhat_0,ite) :\n",
    "        if yhat_0[:,0].sum()*2>len(yhat_0):\n",
    "            minority = xhat_0[yhat_0[:,0]==0]\n",
    "            minority_label = yhat_0[yhat_0[:,0]==0]\n",
    "            minority_ite = ite[yhat_0[:,0]==0]\n",
    "        else:\n",
    "            minority = xhat_0[yhat_0[:,0]==1]\n",
    "            minority_label = yhat_0[yhat_0[:,0]==1]\n",
    "            minority_ite = ite[yhat_0[:,0]==0]\n",
    "        upsampled_data = [xhat_0]\n",
    "        upsampled_label = [yhat_0]\n",
    "        upsampled_ite = [ite]\n",
    "        for i,sample_ind in enumerate(np.array_split(np.arange(len(minority)),100)):\n",
    "            upsampled_data.append(self.generate_fake(minority[sample_ind,:]))\n",
    "            upsampled_label.append(minority_label[sample_ind,:].repeat(self.sample_ratio,1))\n",
    "            upsampled_ite.append(minority_ite[sample_ind].repeat(self.sample_ratio))\n",
    "        epoch_data = torch.cat(upsampled_data, dim=0)\n",
    "        epoch_label = torch.cat(upsampled_label, dim=0)\n",
    "        epoch_ite = torch.cat(upsampled_ite, dim=0)\n",
    "        return epoch_data,epoch_label ,epoch_ite    \n",
    "    def pred_loss(self,targets,out_encoder):\n",
    "        # when classification: cn_loss = nn.BCEWithLogitsLoss().cuda()\n",
    "        pred_losses = []       \n",
    "        if len(targets[targets[:,0]==1,1])>0:\n",
    "            loc = out_encoder[targets[:,0]==1,1].reshape((-1,1)) *self.std_treat + self.mean_treat\n",
    "            truth = targets[targets[:,0]==1,1].reshape((-1,1)) *self.std_treat + self.mean_treat\n",
    "            pred_losses.append(self.mse(loc,truth))\n",
    "        else:\n",
    "            pred_losses.append(torch.tensor([float('0')]).to(self.device))\n",
    "        if len(targets[targets[:,0]==0,2])>0:\n",
    "            loc = out_encoder[targets[:,0]==0,2].reshape((-1,1))*self.std_control + self.mean_control\n",
    "            truth = targets[targets[:,0]==0,2].reshape((-1,1))*self.std_control + self.mean_control\n",
    "            pred_losses.append(self.mse(loc,truth))\n",
    "        else:\n",
    "            pred_losses.append(torch.tensor([float('0')]).to(self.device))\n",
    "        return pred_losses\n",
    "    def compute_mmd(self,z1, z2, reg_weight):\n",
    "        prior_z__kernel = self.compute_kernel(z1, z1)\n",
    "        z__kernel = self.compute_kernel(z2, z2)\n",
    "        priorz_z__kernel = self.compute_kernel(z1, z2)\n",
    "\n",
    "        mmd = reg_weight * prior_z__kernel.mean() + \\\n",
    "              reg_weight * z__kernel.mean() - \\\n",
    "              2 * reg_weight * priorz_z__kernel.mean()\n",
    "        return mmd\n",
    "    def compute_kernel(self,x1,x2,kernel_type='rbf'):\n",
    "        # Convert the tensors into row and column vectors\n",
    "        D = x1.size(1)\n",
    "        N = x1.size(0)\n",
    "        x1 = x1.unsqueeze(-2) # Make it into a column tensor\n",
    "        x2 = x2.unsqueeze(-3) # Make it into a row tensor\n",
    "\n",
    "        \"\"\"\n",
    "        Usually the below lines are not required, especially in our case,\n",
    "        but this is useful when x1 and x2 have different sizes\n",
    "        along the 0th dimension.\n",
    "        \"\"\"\n",
    "        x1 = x1.expand(N, N, D)\n",
    "        x2 = x2.expand(N, N, D)\n",
    "        if kernel_type == 'rbf':\n",
    "            result = self.compute_rbf(x1, x2)\n",
    "        elif kernel_type == 'imq':\n",
    "            result = self.compute_inv_mult_quad(x1, x2)\n",
    "        else:\n",
    "            raise ValueError('Undefined kernel type.')\n",
    "        return result\n",
    "\n",
    "    def compute_rbf(self,x1,x2,eps = 1e-7):\n",
    "        \"\"\"\n",
    "        Computes the RBF Kernel between x1 and x2.\n",
    "        :param x1: (Tensor)\n",
    "        :param x2: (Tensor)\n",
    "        :param eps: (Float)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        z_dim = x2.size(-1)\n",
    "        sigma = 2. * z_dim * self.z_var\n",
    "        result = torch.exp(-((x1 - x2).pow(2).mean(-1) / sigma))\n",
    "        return result\n",
    "\n",
    "    def compute_inv_mult_quad(self,x1,x2,eps= 1e-7):\n",
    "        \"\"\"\n",
    "        Computes the Inverse Multi-Quadratics Kernel between x1 and x2,\n",
    "        given by\n",
    "                k(x_1, x_2) = \\sum \\frac{C}{C + \\|x_1 - x_2 \\|^2}\n",
    "        :param x1: (Tensor)\n",
    "        :param x2: (Tensor)\n",
    "        :param eps: (Float)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        z_dim = x2.size(-1)\n",
    "        C = 2 * z_dim * self.z_var\n",
    "        kernel = C / (eps + C + (x1 - x2).pow(2).sum(dim = -1))\n",
    "        # Exclude diagonal elements\n",
    "        result = kernel.sum() - kernel.diag().sum()\n",
    "        return result\n",
    "    def _trainer(self, train_data, targets):\n",
    "        train_data=train_data.view(-1, self.input_dimension)\n",
    "        targets = targets.view(-1,self.number_of_labels)\n",
    "        self.batch_size = train_data.shape[0]\n",
    "        torch.cuda.empty_cache()\n",
    "        self.optimizer.zero_grad()\n",
    "        self.encoder.zero_grad()\n",
    "        self.decoder.zero_grad()\n",
    "        [\n",
    "            loss,\n",
    "            pred_loss,\n",
    "            recon_loss,\n",
    "            original_KL,\n",
    "            tc_loss,\n",
    "            mmd_loss,\n",
    "        ] = self.compute_loss(data=train_data,targets = targets)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return (\n",
    "            loss.item(),\n",
    "            pred_loss,\n",
    "            recon_loss,\n",
    "            original_KL,\n",
    "            tc_loss,\n",
    "            mmd_loss\n",
    "        )\n",
    "    def trainer(self,train_data,test_data,train_label,eval_data, ite_train, ite_eval,ite_test,\n",
    "                ):\n",
    "        self.encoder.apply(self.weights_init)\n",
    "        self.decoder.apply(self.weights_init)\n",
    "        train_loss = []\n",
    "        val_loss = []\n",
    "        test_score=-1\n",
    "        \n",
    "        train_data = torch.Tensor(train_data)\n",
    "        test_data = torch.Tensor(test_data).to(self.device)\n",
    "        train_label = torch.Tensor(train_label)\n",
    "        eval_data = torch.Tensor(eval_data).to(self.device)\n",
    "        ite_train = torch.Tensor(ite_train)\n",
    "        ite_eval = torch.Tensor(ite_eval).to(self.device)\n",
    "        ite_test = torch.Tensor(ite_test).to(self.device)\n",
    "        \n",
    "        self.mean_treat  = torch.mean(train_label[:,1])\n",
    "        self.std_treat  = torch.std(train_label[:,1])\n",
    "        self.mean_control = torch.mean(train_label[:,2])\n",
    "        self.std_control = torch.std(train_label[:,2])\n",
    "        train_label[:,1] = (train_label[:,1]- self.mean_treat) / self.std_treat\n",
    "        train_label[:,2] = (train_label[:,2] - self.mean_control) / self.std_control\n",
    "        epoch_label = train_label.detach().clone()\n",
    "        epoch_data = train_data.detach().clone()\n",
    "        epoch_ite = ite_train.detach().clone()\n",
    "        for epoch in range(self.n_epochs):\n",
    "            self.epoch = epoch\n",
    "            epoch_train_loss=[]\n",
    "            train_set = torch.utils.data.TensorDataset(epoch_data, epoch_label,epoch_ite)\n",
    "            train_loader = DataLoader(train_set, shuffle=True,\n",
    "                                      num_workers=1, drop_last=True, \n",
    "                                      batch_size=self.batch_size)\n",
    "            self.n_data = len(epoch_data)\n",
    "            for i, batch_data in enumerate(train_loader,0):\n",
    "                data,label,ite = batch_data\n",
    "                loss, pred_loss, recon_loss,original_KL,tc_loss, mmd_loss = self._trainer(data.to(self.device),\n",
    "                                                                                          label.to(self.device),\n",
    "                                                                                         )\n",
    "                epoch_train_loss.append([loss, pred_loss,recon_loss,original_KL,\n",
    "                                         tc_loss, mmd_loss])\n",
    "            \n",
    "            epoch_pred = self.encoder(eval_data)\n",
    "            epoch_pred[:,1] = epoch_pred[:,1] *self.std_treat + self.mean_treat\n",
    "            epoch_pred[:,2] = epoch_pred[:,2] *self.std_control + self.mean_control\n",
    "            preds= epoch_pred[:,1] - epoch_pred[:,2]\n",
    "            \n",
    "            epoch_val_loss=self.mse(preds.reshape(-1,1) ,\n",
    "                                    ite_eval.reshape(-1,1)\n",
    "                                   ).item()**0.5 \n",
    "            train_loss.append(np.mean(epoch_train_loss,axis=0))\n",
    "            val_loss.append(epoch_val_loss)      \n",
    "            if self.generate_data:\n",
    "                with torch.no_grad():\n",
    "                    epoch_data,epoch_label,epoch_ite = self.new_data(train_data.detach().clone(),\n",
    "                                                           train_label.detach().clone(),\n",
    "                                                          ite_train.detach().clone())   \n",
    "            if self.early_stopper.early_stop(epoch_val_loss):\n",
    "                encoded_test = self.encoder(test_data) \n",
    "                encoded_test[:,1] = encoded_test[:,1] *self.std_treat + self.mean_treat\n",
    "                encoded_test[:,2] = encoded_test[:,2] *self.std_control + self.mean_control\n",
    "                preds= encoded_test[:,1] - encoded_test[:,2]\n",
    "                test_score = self.mse(preds.reshape(-1,1),\n",
    "                                      ite_test.reshape(-1,1)).item()**0.5\n",
    "                break\n",
    "        if self.plot:\n",
    "            f = plt.figure(figsize=(10,5))\n",
    "            train_losses= np.array(train_loss).reshape(-1,6)\n",
    "            for i, loss in enumerate(['combined_loss','pred_loss','recon_loss','original KL',\n",
    "                                      'tc_loss','mmd_loss'+str(self.wasserstein)]):\n",
    "                train = train_losses[:,i]\n",
    "                ax = f.add_subplot(1,1,1)\n",
    "                plt.title(loss)\n",
    "                plt.plot(train,label='train')\n",
    "                if i ==1:\n",
    "                    plt.plot(val_loss,label='eval' )\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "        return train_loss,val_loss,test_score\n",
    "    def simple_mmd_loss(self,X_treat, X_control):\n",
    "        \"\"\"Calculate Maximum Mean Discrepancy loss.\"\"\"\n",
    "        return 2 * torch.norm(X_treat.mean(axis=0) - X_control.mean(axis=0))\n",
    "\n",
    "    def compute_loss(self, data,targets):\n",
    "        out_encoder = self.encoder(data)\n",
    "        #resample latent variables \n",
    "        q_zgivenxobs = td.Independent(\n",
    "            td.Normal(\n",
    "                loc=out_encoder[..., : self.latent_dimension],\n",
    "                scale=torch.exp(out_encoder[..., self.latent_dimension :]) ** 0.5,\n",
    "            ),\n",
    "            1,\n",
    "        )  # each row is a latent vector\n",
    "        zgivenx_flat = q_zgivenxobs.rsample()\n",
    "        zgivenx = zgivenx_flat.reshape((-1, self.latent_dimension))\n",
    "\n",
    "        # calculate reconstruction loss\n",
    "        out_decoder = self.decoder(zgivenx)\n",
    "        recon_loss = self.mse(out_decoder, data)\n",
    "        \n",
    "        #calculate mmd_loss\n",
    "        if len(targets[targets[:,0]==1,:])==0 or len(targets[targets[:,0]==1,:])==len(targets):\n",
    "            mmd_loss = torch.tensor([float('0')]).to(self.device)\n",
    "        else:\n",
    "            control = td.Independent(td.Normal(loc=out_encoder[targets[:,0]==0, \n",
    "                                                               3:self.latent_dimension],\n",
    "                                                scale=torch.exp(\n",
    "                                                    out_encoder[targets[:,0]==0,\n",
    "                                                                self.latent_dimension+3 :]) ** 0.5,\n",
    "                                                )\n",
    "                                       ,1)\n",
    "            #x1 shape: (K*bs, 1, latent_dim)\n",
    "            x1 = control.rsample([self.K]).view(-1,1,\n",
    "                                           self.latent_dimension-self.number_of_labels)\n",
    "            \n",
    "            target = control.log_prob(x1).mean(axis=1).view(-1,1) \n",
    "            #target shape: (K*bs, control.size(0))\n",
    "            treatment = td.Independent(td.Normal(loc=out_encoder[targets[:,0]==1, \n",
    "                                                                 3:self.latent_dimension],\n",
    "                                                scale=torch.exp(\n",
    "                                                    out_encoder[targets[:,0]==1, \n",
    "                                                                self.latent_dimension+3 :]) ** 0.5,\n",
    "                                                )\n",
    "                                       ,1)\n",
    "            #prob_treatment shape: (K*bs, treatment.size(0))\n",
    "            prob_treatment = treatment.log_prob(x1).mean(axis=1).view(-1,1)\n",
    "            if self.wasserstein==1: \n",
    "                bias_corr = self.batch_size *  (self.batch_size - 1)\n",
    "                reg_weight = self.reg_weight / bias_corr\n",
    "                mmd_loss = 10**6*self.compute_mmd(target, \n",
    "                                            prob_treatment,\n",
    "                                            reg_weight)\n",
    "            elif self.wasserstein==2: \n",
    "                mmd_loss=10*self.simple_mmd_loss(target,\n",
    "                                              prob_treatment)\n",
    "                \n",
    "            elif self.wasserstein==3:\n",
    "                mmd_loss, _,_ = self.sinkhorn(target, \n",
    "                                              prob_treatment)\n",
    "                mmd_loss = 10*mmd_loss\n",
    "            else: \n",
    "                mmd_loss = 10**8*self.kl_loss(F.log_softmax(torch.exp(prob_treatment),dim=0),\n",
    "                                        F.log_softmax(torch.exp(target),dim=0)\n",
    "                                        )\n",
    "       \n",
    "        # calculate the original KL in VAE\n",
    "        original_KL = self._kl_normal_loss(\n",
    "            out_encoder[..., self.number_of_labels: self.latent_dimension],\n",
    "            out_encoder[..., self.latent_dimension +self.number_of_labels :],\n",
    "        )\n",
    "        # prob of z given observations x\n",
    "        log_pz = (\n",
    "            td.Independent(\n",
    "                td.Normal(\n",
    "                    loc=torch.zeros_like(zgivenx), scale=torch.ones_like(zgivenx)\n",
    "                ),\n",
    "                1,\n",
    "            )\n",
    "            .log_prob(zgivenx)\n",
    "            .mean()\n",
    "        )\n",
    "        log_q_zCx = q_zgivenxobs.log_prob(zgivenx).mean()\n",
    "\n",
    "        log_qz, log_prod_qzi = self.get_log_qz_prodzi(\n",
    "            zgivenx, out_encoder\n",
    "        )\n",
    "        # I[z;x] = KL[q(z,x)||q(x)q(z)] = E_x[KL[q(z|x)||q(z)]]\n",
    "        mi_loss = (log_q_zCx - log_qz).mean()\n",
    "        # TC[z] = KL[q(z)||\\prod_i z_i]\n",
    "        tc_loss = (log_qz - log_prod_qzi).mean()\n",
    "        # dw_kl_loss is KL[q(z)||p(z)] instead of usual KL[q(z|x)||p(z))]\n",
    "        dw_kl_loss = (log_prod_qzi - log_pz).mean()\n",
    "        \n",
    "        prediction_losses = self.pred_loss(targets,out_encoder)\n",
    "        \n",
    "        loss_prediction=  (1 + 0.2*self.epoch)*(prediction_losses[1]*self.pred_weight[1] + \\\n",
    "        prediction_losses[0]*self.pred_weight[0])\n",
    "        neg_bound =  loss_prediction+ recon_loss*self.recon_weight + original_KL*self.KL_weight \\\n",
    "        + tc_loss * self.beta + mmd_loss*self.gamma\n",
    "        return (\n",
    "            neg_bound,\n",
    "            loss_prediction.item(),\n",
    "            recon_loss.item()*self.recon_weight,\n",
    "            original_KL.item()*self.KL_weight,\n",
    "            tc_loss.item()* self.beta,\n",
    "            mmd_loss.item()*self.gamma\n",
    "        )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6313a1e-e075-4a84-b698-cd6dff84bd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "path = 'csv_files/' #path of IHDP files, downloaded from https://www.fredjo.com/\n",
    "def processed_data(i):\n",
    "    train_data = pd.read_csv(path+'1000_train'+str(i)+'.csv').values\n",
    "    train_data[:,13] = train_data[:,13]-1 #processing according to CEVAE\n",
    "    test_data = pd.read_csv(path+'1000_test'+str(i)+'.csv').values\n",
    "    test_data[:,13] = test_data[:,13]-1 #processing according to CEVAE\n",
    "    train_label = np.zeros((len(train_data),3))\n",
    "    t= pd.read_csv(path+'1000_train_t'+str(i)+'.csv').values.flatten()\n",
    "    train_label[:,0] = t\n",
    "    yf = pd.read_csv(path+'1000_train_yf'+str(i)+'.csv').values.flatten()\n",
    "    ycf = pd.read_csv(path+'1000_train_ycf'+str(i)+'.csv').values.flatten()\n",
    "    train_label[:,1] = np.where(t==1,yf, ycf) #treatment effect\n",
    "    train_label[:,2] = np.where(t==0,yf, ycf) #non-treatment effect\n",
    "    train_ite = np.zeros((len(train_data),1))\n",
    "    train_ite = np.where(t==1,yf-ycf, ycf-yf)\n",
    "    ite_test = pd.read_csv(path+'1000_test_ite'+str(i)+'.csv').values\n",
    "    train_eval_split = int(0.8*len(train_data))\n",
    "    indices = np.random.permutation(train_data.shape[0])\n",
    "    training_idx, eval_idx = indices[:train_eval_split], indices[train_eval_split:]\n",
    "    return train_data[training_idx,:],test_data,train_label[training_idx,:],\\\n",
    "            train_data[eval_idx,:],train_ite[training_idx],train_ite[eval_idx], \\\n",
    "            ite_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "769420f5-c382-4eea-8227-78e9158a212b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "VAE = DisentangledVAE(n_epochs=150, number_of_labels=3,input_dimension =25 ,\n",
    "                      latent_dimension = 10, \n",
    "                      hidden_layer_width=500,device=device)\n",
    "VAE.batch_size=32\n",
    "VAE.optimizer = optim.Adam(\n",
    "    list(VAE.encoder.parameters()) + list(VAE.decoder.parameters()), lr=1e-4\n",
    "    # ,weight_decay=1e-5\n",
    ")\n",
    "#prediction loss\n",
    "VAE.pred_weight = [1,1]\n",
    "# KL loss\n",
    "VAE.KL_weight=1\n",
    "#TC loss\n",
    "VAE.beta = .1\n",
    "#mmd loss\n",
    "VAE.gamma=.1\n",
    "VAE.plot=False\n",
    "#recon loss\n",
    "VAE.recon_weight = .1\n",
    "\n",
    "VAE.early_stopper = EarlyStopper(patience=4, min_delta=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63e35a55-3e71-4f10-8574-d180a0c78a99",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01171112060546875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 24,
       "postfix": null,
       "prefix": "replication",
       "rate": null,
       "total": 1000,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4346f997a27047a69971d30bb792213f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replication:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7558301579942495\n",
      "20 1.1997330698872024\n",
      "40 1.4015571646085876\n",
      "60 1.2460875413109855\n",
      "80 1.16341406855023\n",
      "100 1.1909069341294314\n",
      "120 1.1736943499885182\n",
      "140 1.174061009167624\n",
      "160 1.161895283457155\n",
      "180 1.1269594246207764\n",
      "200 1.0835938837892087\n",
      "220 1.1104491536499124\n",
      "240 1.0839014553708495\n",
      "260 1.1039305370824604\n",
      "280 1.0876450563651754\n",
      "300 1.0785998109015622\n",
      "320 1.084301519174837\n",
      "340 1.109668150221493\n",
      "360 1.134661119247293\n",
      "380 1.1261475927454196\n",
      "400 1.1242566855494145\n",
      "420 1.1248608941387763\n",
      "440 1.126160569088464\n",
      "460 1.114024931802719\n",
      "480 1.1267691322525049\n",
      "500 1.1176694250793278\n",
      "520 1.122576967763496\n",
      "540 1.1151985180076633\n",
      "560 1.106914793126009\n",
      "580 1.098937150577528\n",
      "600 1.1238941403263083\n",
      "620 1.1225157900381366\n",
      "640 1.1235515792763264\n",
      "660 1.1393948576810426\n",
      "680 1.1469535552016352\n",
      "700 1.1457089801705835\n",
      "720 1.1439502751905228\n",
      "740 1.140806109263491\n",
      "760 1.1373349094842602\n",
      "780 1.162481946892535\n",
      "800 1.1634805794643193\n",
      "820 1.1757626732643385\n",
      "840 1.1769200244935225\n",
      "860 1.171856044557904\n",
      "880 1.173407228449422\n",
      "900 1.175188302567926\n",
      "920 1.1733926222393996\n",
      "940 1.1790050921630142\n",
      "960 1.1832574382286638\n",
      "980 1.1952380697728087\n"
     ]
    }
   ],
   "source": [
    "train_losses=[]\n",
    "eval_losses=[]\n",
    "test_losses=[]\n",
    "\n",
    "#in this demo, we take all 1000 replications\n",
    "for i in trange(1000, position=0, desc=\"replication\", leave=True, colour='black',):\n",
    "    train_data,test_data,train_label,eval_data, ite_train, ite_eval,ite_test = processed_data(i)   \n",
    "    VAE.wasserstein = 3\n",
    "    VAE.pred_weight[0] =min(1, 0.1*train_label[train_label[:,0]==1,1].std())\n",
    "    VAE.pred_weight[1] =min(1, 0.1*train_label[train_label[:,0]==0,2].std())\n",
    "    VAE.early_stopper = EarlyStopper(patience=3, min_delta=0.05)\n",
    "    score_train,score_eval,test_score = VAE.trainer(train_data,test_data,train_label,\n",
    "                                                    eval_data, ite_train, ite_eval,\n",
    "                                                    ite_test)\n",
    "    train_losses.append(score_train)\n",
    "    eval_losses.append(score_eval)\n",
    "    test_losses.append(test_score)\n",
    "    if i%20==0:\n",
    "        print(i, np.mean(test_losses))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7ddfdf1-2eaa-465b-90ff-0cfb1a059904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The average rpehe of test data are:  1.2000013613424507 The standard error of epehe in test data are:  0.0409820255155511\n"
     ]
    }
   ],
   "source": [
    "#print the results\n",
    "from scipy.stats import sem\n",
    "results = np.array(test_losses)\n",
    "print(\n",
    "    '\\n The average rpehe of test data are: ',\n",
    "    np.mean(results),\n",
    "    'The standard error of epehe in test data are: ',\n",
    "    sem(results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
